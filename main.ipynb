{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model ai-forever/mGPT into HookedTransformer\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformer_lens\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"ai-forever/mGPT\", device=device)\n",
    "print(model.cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'NTK_by_parts_factor': 8.0,\n",
      " 'NTK_by_parts_high_freq_factor': 4.0,\n",
      " 'NTK_by_parts_low_freq_factor': 1.0,\n",
      " 'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': np.float64(11.313708498984761),\n",
      " 'attn_scores_soft_cap': -1.0,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 8192,\n",
      " 'd_model': 2048,\n",
      " 'd_vocab': 100000,\n",
      " 'd_vocab_out': 100000,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='mps'),\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': np.float64(0.017677669529663688),\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'mGPT',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 16,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 24,\n",
      " 'n_params': 1207959552,\n",
      " 'normalization_type': 'LNPre',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'output_logits_soft_cap': -1.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'ai-forever/mGPT',\n",
      " 'tokenizer_prepends_bos': True,\n",
      " 'trust_remote_code': False,\n",
      " 'ungroup_grouped_query_attention': False,\n",
      " 'use_NTK_by_parts_rope': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_normalization_before_and_after': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "print(model.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from transformer_lens import ActivationCache\n",
    "\n",
    "\n",
    "def collect_activation_cache(data: List[dict[str, str]]):\n",
    "    activation_cache: dict[str, List[ActivationCache]] = {}\n",
    "    for entry in data:\n",
    "        for language, text in entry.items():\n",
    "            if language not in activation_cache:\n",
    "                activation_cache[language] = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tokens = model.to_tokens(text)\n",
    "                logits, cache = model.run_with_cache(tokens)\n",
    "                activation_cache[language].append(cache)\n",
    "\n",
    "    return activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def collect_hidden_space_by_language(activation_cache: dict[str, List[ActivationCache]]):\n",
    "    hidden_space_for_language = {}\n",
    "\n",
    "    for language, language_caches in activation_cache.items():\n",
    "        # d_model, n_prompts, n_layers\n",
    "        hidden_space_for_language_by_layer = np.zeros(\n",
    "            (model.cfg.d_model, len(language_caches), model.cfg.n_layers)\n",
    "        )\n",
    "\n",
    "        for cache_i, cache in enumerate(language_caches):\n",
    "            # layer, batch, pos, d_model\n",
    "            accum_resid = cache.accumulated_resid(apply_ln=True)\n",
    "            hidden_space_for_language_by_layer[:, cache_i, :] = accum_resid[1:, 0, -1, :].cpu().numpy().T\n",
    "\n",
    "        hidden_space_for_language[language] = hidden_space_for_language_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = [{\"EN\": \"Hello\", \"RU\": \"Здравствуйте\"}]\n",
    "activation_cache = collect_activation_cache(toy_data)\n",
    "hidden_space_by_language = collect_hidden_space_by_language(activation_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
